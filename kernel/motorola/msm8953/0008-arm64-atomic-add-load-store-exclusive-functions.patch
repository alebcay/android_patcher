From b37a76927921c3fdae5ec1cd2b61e19f7f3173cb Mon Sep 17 00:00:00 2001
From: Alex Van Brunt <avanbrunt@nvidia.com>
Date: Wed, 29 Jan 2014 13:33:17 -0800
Subject: [PATCH 08/27] arm64: atomic: add load/store exclusive functions

Add functions that wrap the load and store exlusive functions so that
non-assemby code can use it.

Change-Id: I223f085bec454a156bef1ca659540cdd5c8e521f
Signed-off-by: Alex Van Brunt <avanbrunt@nvidia.com>
Reviewed-on: http://git-master/r/361735
Reviewed-by: Peng Du <pdu@nvidia.com>
Reviewed-on: http://git-master/r/365058
Reviewed-by: Richard Wiley <rwiley@nvidia.com>
Tested-by: Oskari Jaaskelainen <oskarij@nvidia.com>
Signed-off-by: Jesse Chan <jc@linux.com>
Signed-off-by: Michele Beccalossi <beccalossi.michele@gmail.com>
---
 arch/arm64/include/asm/atomic.h | 144 ++++++++++++++++++++++++++++++++++++++++
 1 file changed, 144 insertions(+)

diff --git a/arch/arm64/include/asm/atomic.h b/arch/arm64/include/asm/atomic.h
index c1c3417af5b..02b19d6a18f 100644
--- a/arch/arm64/include/asm/atomic.h
+++ b/arch/arm64/include/asm/atomic.h
@@ -38,6 +38,150 @@
 #define atomic_read(v)	ACCESS_ONCE((v)->counter)
 #define atomic_set(v,i)	(((v)->counter) = (i))
 
+static inline u64 ldx64(volatile u64 *p)
+{
+	u64 ret;
+	asm volatile("ldxr %x0, %1" : "=&r" (ret) : "Q" (*p) : "memory");
+	return ret;
+}
+
+static inline int stx64(volatile u64 *p, u64 v)
+{
+	int ret;
+	asm volatile(
+		 "stxr %x0, %x1, %2"
+		: "=&r" (ret)
+		: "r" (v), "Q" (*p)
+		: "memory");
+	return ret;
+}
+
+static inline u32 ldx32(volatile u32 *p)
+{
+	u32 ret;
+	asm volatile("ldxr %w0, %1" : "=&r" (ret) : "Q" (*p) : "memory");
+	return ret;
+}
+
+static inline int stx32(volatile u32 *p, u32 v)
+{
+	int ret;
+	asm volatile(
+		 "stxr %w0, %w1, %2"
+		: "=&r" (ret)
+		: "r" (v), "Q" (*p)
+		: "memory");
+	return ret;
+}
+
+static inline u16 ldx16(volatile u16 *p)
+{
+	u16 ret;
+	asm volatile("ldxrh %w0, %1" : "=&r" (ret) : "Q" (*p) : "memory");
+	return ret;
+}
+
+static inline int stx16(volatile u16 *p, u16 v)
+{
+	int ret;
+	asm volatile(
+		 "stxrh %w0, %w1, %2"
+		: "=&r" (ret)
+		: "r" (v), "Q" (*p)
+		: "memory");
+	return ret;
+}
+
+static inline u8 ldx8(volatile u8 *p)
+{
+	u8 ret;
+	asm volatile("ldxrb %w0, %1" : "=&r" (ret) : "Q" (*p) : "memory");
+	return ret;
+}
+
+static inline int stx8(volatile u8 *p, u8 v)
+{
+	int ret;
+	asm volatile(
+		 "stxrb %w0, %w1, %2"
+		: "=&r" (ret)
+		: "r" (v), "Q" (*p)
+		: "memory");
+	return ret;
+}
+
+static inline u64 ldax64(volatile u64 *p)
+{
+	u64 ret;
+	asm volatile("ldaxr %x0, %1" : "=&r" (ret) : "Q" (*p) : "memory");
+	return ret;
+}
+
+static inline int stlx64(volatile u64 *p, u64 v)
+{
+	int ret;
+	asm volatile(
+		 "stlxr %x0, %x1, %2"
+		: "=&r" (ret)
+		: "r" (v), "Q" (*p)
+		: "memory");
+	return ret;
+}
+
+static inline u32 ldax32(volatile u32 *p)
+{
+	u32 ret;
+	asm volatile("ldaxr %w0, %1" : "=&r" (ret) : "Q" (*p) : "memory");
+	return ret;
+}
+
+static inline int stlx32(volatile u32 *p, u32 v)
+{
+	int ret;
+	asm volatile(
+		 "stlxr %w0, %w1, %2"
+		: "=&r" (ret)
+		: "r" (v), "Q" (*p)
+		: "memory");
+	return ret;
+}
+
+static inline u16 ldax16(volatile u16 *p)
+{
+	u16 ret;
+	asm volatile("ldaxrh %w0, %1" : "=&r" (ret) : "Q" (*p) : "memory");
+	return ret;
+}
+
+static inline int stlx16(volatile u16 *p, u16 v)
+{
+	int ret;
+	asm volatile(
+		 "stlxrh %w0, %w1, %2"
+		: "=&r" (ret)
+		: "r" (v), "Q" (*p)
+		: "memory");
+	return ret;
+}
+
+static inline u8 ldax8(volatile u8 *p)
+{
+	u8 ret;
+	asm volatile("ldaxrb %w0, %1" : "=&r" (ret) : "Q" (*p) : "memory");
+	return ret;
+}
+
+static inline int stlx8(volatile u8 *p, u8 v)
+{
+	int ret;
+	asm volatile(
+		 "stlxrb %w0, %w1, %2"
+		: "=&r" (ret)
+		: "r" (v), "Q" (*p)
+		: "memory");
+	return ret;
+}
+
 /*
  * AArch64 UP and SMP safe atomic ops.  We use load exclusive and
  * store exclusive to ensure that these are atomic.  We may loop
-- 
2.11.0

