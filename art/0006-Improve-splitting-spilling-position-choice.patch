From c88e344f9a88ed9b8d5103e5e6673bca936e8262 Mon Sep 17 00:00:00 2001
From: Artem Serov <artem.serov@linaro.org>
Date: Mon, 13 Mar 2017 11:43:00 +0000
Subject: [PATCH 6/6] Improve splitting/spilling position choice.

Improve splitting/spilling position choice for a particular case
when "active" interval is split exactly at the start position of
"current" interval regardless its previous uses, loop structure,
etc. The patch fixes the problem when a value is reloaded just
before the back edge of a loop and has no uses inside and thus is
redundant. This happens when interval is split in the middle of the
loop however split position can be hoisted out of it. The case is
exploited only when it is safe to change the split point and it
doesn't break any invariants of linear scan algorithm.

On ARM (32bit) the patch:
 * reduces the number of spill/fill loads by up to 23%.
 * brings up to 15% (LoopAtom) performance improvement.
 * brings up to 2.5% .text size reduction.

Test: mma test-art-host && mma test-art-target

Change-Id: I6a0c776de41785a7734f3cf0b68c46a3a76be666

Signed-off-by: Pranav Vashi <neobuddy89@gmail.com>
---
 .../optimizing/register_allocator_linear_scan.cc   | 83 ++++++++++++++++++++--
 .../optimizing/register_allocator_linear_scan.h    |  1 +
 compiler/optimizing/ssa_liveness_analysis.h        | 31 ++++++++
 3 files changed, 108 insertions(+), 7 deletions(-)

diff --git a/compiler/optimizing/register_allocator_linear_scan.cc b/compiler/optimizing/register_allocator_linear_scan.cc
index ab8d540359..a0ecb86afd 100644
--- a/compiler/optimizing/register_allocator_linear_scan.cc
+++ b/compiler/optimizing/register_allocator_linear_scan.cc
@@ -858,6 +858,81 @@ bool RegisterAllocatorLinearScan::TrySplitNonPairOrUnalignedPairIntervalAt(size_
   return false;
 }
 
+// Process (split/spill, update register allocator lists) active interval which has the register
+// the current interval has.
+void RegisterAllocatorLinearScan::ProcessActiveHoldingReg(ArenaVector<LiveInterval*>::iterator it,
+    LiveInterval* current) {
+  LiveInterval* active = *it;
+  DCHECK_EQ(active->GetRegister(), current->GetRegister());
+  DCHECK(!active->IsFixed());
+  size_t last_use_pos = active->LastUseBefore(current->GetStart());
+  // An interval should be at least 1 position long.
+  size_t after_last_use_pos = active->GetStart() ? last_use_pos + 1 : last_use_pos;
+  LiveInterval* split = nullptr;
+  // At "current->GetStart()" position an active interval AI_orig should be spilled. Instead of
+  // splitting and spilling at this exact position we try to find an earlier split point
+  // which is more beneficial. Then after splitting the active interval (AI_orig -> AI_1 + AI_2)
+  // two cases are possible (V - is "current->GetStart()", x - register use):
+  //
+  //   1. the second part AI_2 has no register uses:
+  //     - we can safely spill it.
+  //                 V
+  //        |....x...............|      AI_orig
+  //
+  //                 V
+  //        |....x|                     AI_1
+  //              |..............|      AI_2 - spilled
+  //
+  //   2. the second part AI_2 has some register uses:
+  //     - we have to split again (AI_2 -> AI_2_1 + AI_2_2): the middle part AI_2_1 will be
+  //       spilled, the third one AI_2_2 will be inserted into unhandled_ set.
+  //                 V
+  //        |....x........x......|      AI_orig
+  //
+  //                 V
+  //        |....x|                     AI_1
+  //              |......|              AI_2_1 - spilled
+  //                     |x......|      AI_2_2 - added to unhandled_
+  //
+  //     - Though we get an interval AI_2_1 with a start position less than the current position we
+  //       don't add it to any sets, immediately process it and never backtrack. So the start
+  //       position of the current in LinearScan loop current->GetStart() never declines.
+  if (after_last_use_pos >= current->GetStart()) {
+    // Default case.
+    split = Split(active, current->GetStart());
+    AddSorted(unhandled_, split);
+  } else {
+    // Optimized case.
+    split = SplitBetween(active, after_last_use_pos, current->GetStart());
+
+    if (split->GetStart() == current->GetStart()) {
+      AddSorted(unhandled_, split);
+    } else {
+      LiveInterval* split2 = nullptr;
+      size_t first_reg_use = split->FirstRegisterUse();
+      DCHECK_EQ(split->FirstRegisterUseAfter(after_last_use_pos), first_reg_use);
+      if (split->FirstRegisterUse() != kNoLifetime) {
+        // In order to get an interval that can be spilled we should split the original one
+        // BEFORE its first register use.
+        size_t split_pos = first_reg_use - 1;
+        DCHECK_GE(split_pos, current->GetStart());
+        split2 = Split(split, split_pos);
+        AddSorted(unhandled_, split2);
+      }
+
+      DCHECK(!split->HasRegister());
+      DCHECK(split->FirstRegisterUse() == kNoLifetime);
+      // To spill a pair interval we need to spill its LowInterval; high one will be processed
+      // automatically.
+      AllocateSpillSlotFor(active->IsHighInterval() ? split->GetLowInterval() : split);
+    }
+  }
+  if (split != active) {
+    handled_.push_back(active);
+  }
+  RemoveIntervalAndPotentialOtherHalf(&active_, it);
+}
+
 // Find the register that is used the last, and spill the interval
 // that holds it. If the first use of `current` is after that register
 // we spill `current` instead.
@@ -990,13 +1065,7 @@ bool RegisterAllocatorLinearScan::AllocateBlockedReg(LiveInterval* current) {
     for (auto it = active_.begin(), end = active_.end(); it != end; ++it) {
       LiveInterval* active = *it;
       if (active->GetRegister() == reg) {
-        DCHECK(!active->IsFixed());
-        LiveInterval* split = Split(active, current->GetStart());
-        if (split != active) {
-          handled_.push_back(active);
-        }
-        RemoveIntervalAndPotentialOtherHalf(&active_, it);
-        AddSorted(unhandled_, split);
+        ProcessActiveHoldingReg(it, current);
         break;
       }
     }
diff --git a/compiler/optimizing/register_allocator_linear_scan.h b/compiler/optimizing/register_allocator_linear_scan.h
index b3834f45e4..ec7a27aa39 100644
--- a/compiler/optimizing/register_allocator_linear_scan.h
+++ b/compiler/optimizing/register_allocator_linear_scan.h
@@ -104,6 +104,7 @@ class RegisterAllocatorLinearScan : public RegisterAllocator {
   bool TrySplitNonPairOrUnalignedPairIntervalAt(size_t position,
                                                 size_t first_register_use,
                                                 size_t* next_use);
+  void ProcessActiveHoldingReg(ArenaVector<LiveInterval*>::iterator it, LiveInterval* current);
 
   // List of intervals for core registers that must be processed, ordered by start
   // position. Last entry is the interval that has the lowest start position.
diff --git a/compiler/optimizing/ssa_liveness_analysis.h b/compiler/optimizing/ssa_liveness_analysis.h
index a6681575a2..51677749cc 100644
--- a/compiler/optimizing/ssa_liveness_analysis.h
+++ b/compiler/optimizing/ssa_liveness_analysis.h
@@ -604,6 +604,37 @@ class LiveInterval : public ArenaObject<kArenaAllocSsaLiveness> {
     return parent_->uses_;
   }
 
+  // Returns the position of the interval's last use before the specified position - a position of
+  // a use USE_k:
+  // 1. USE_k <= POSITION
+  // 2. For each USE_i <= POSITION ---> pos(USE_i) <= pos(USE_k)
+  //
+  // Example: LiveInterval li; (ranges: { [134,135) }, uses: { 200 248 255 }, { 137 153 201 })
+  // li->LastUseBefore(250) will return 248.
+  size_t LastUseBefore(size_t position) const {
+    DCHECK(!is_temp_);
+
+    if (IsDefiningPosition(position)) {
+      DCHECK(defined_by_->GetLocations()->Out().IsValid());
+      return position;
+    }
+
+    size_t res_pos = GetStart();
+    size_t end_pos = std::min(position, GetEnd());
+    for (const UsePosition& use : GetUses()) {
+      size_t use_position = use.GetPosition();
+      if (use_position > end_pos) {
+        break;
+      }
+      // A split interval holds all uses of its parent so we want to filter those uses.
+      if (use_position >= GetStart()) {
+        res_pos = use_position;
+      }
+    }
+
+    return res_pos;
+  }
+
   const EnvUsePositionList& GetEnvironmentUses() const {
     return parent_->env_uses_;
   }
-- 
2.14.1

